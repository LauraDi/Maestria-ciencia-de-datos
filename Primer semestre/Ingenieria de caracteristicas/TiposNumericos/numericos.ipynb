{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbAtlE33ahZH"
   },
   "source": [
    "<center>\n",
    "<p><img src=\"https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png\" width=\"150\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "# Curso *Ingeniería de Características*\n",
    "\n",
    "### Datos numéricos\n",
    "\n",
    "\n",
    "#### Diana Ballesteros\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WwwzttfLaa27"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jd42qfQdMnq"
   },
   "source": [
    "## Descargando datos\n",
    "\n",
    "Para poder descargar los datos que vamos a utilizar, vamos a requerir el módulo de ``kaggle``. Por lo que vamos a [seguir estos pasos](https://www.kaggle.com/general/74235) para usar correctamente la API de Kaggle en Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gBpOzoMQda0v"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2dfae63ee5d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install -q kaggle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "!pip install -q kaggle\n",
    "\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ImxfNOye8Ad"
   },
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Y ahora probamos que todo funciona bien, al ver la lista de datasets\n",
    "!kaggle datasets list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-AQphFDiKMD"
   },
   "source": [
    "Y listo, vamos a descargar nuestro conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUQ1AxxLiNub"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download 'camnugent/california-housing-prices'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1ybqSnmjb-8"
   },
   "source": [
    "Los datos se encuentran en archivo comprimido, pero es un `csv`, por lo que lo podemos abrir directamente en Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZow6In4jbTM"
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"./california-housing-prices.zip\")\n",
    "\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdM--0aekHBi"
   },
   "source": [
    "## Explorando los datos\n",
    "\n",
    "Vamos a ver como se comportan cada una de las variables y para esto vamos a usar diferentes métodos de graficación que vienen incluidos en Pandas.\n",
    "\n",
    "Lo primero que vamos a hacer va a ser revisar las estadísticas básicas de cada variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIx1LHqysEUn"
   },
   "outputs": [],
   "source": [
    "df_raw.describe(include='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZK-LHN7sduk"
   },
   "source": [
    "Pero esto no nos da toda la información necesaria, por lo que tenemos que revisar cada una de las variables. \n",
    "\n",
    "Vamos a usar un *histograma* para ver como se distribuyen los valores de las diferentes variables.\n",
    "\n",
    "Revisa para cada variable numérica que tenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKsBKNIusr8c"
   },
   "outputs": [],
   "source": [
    "df_raw.longitude.hist(bins=50, grid=False, figsize=(15, 5))\n",
    "print(\"Histograma\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MY7ZAJ2gtkBY"
   },
   "source": [
    "Si quieres ver todas juntas (aunque no muy claro es una ayuda, se puede hacer el histograma de todas las variables numéricas con el siguiente pedazo de codigo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_AJxZA-twZg"
   },
   "outputs": [],
   "source": [
    "hists = df_raw.hist(bins=50, grid=False, figsize=(15, 15))\n",
    "print(\"Histogramas\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qgE6xoEvX19"
   },
   "source": [
    "## Tratamiento de las variables\n",
    "\n",
    "Vamos a modificar las variables pensando en que las vamos a utilizar para desarrollar modelos de regresión y/o clasificacion, y vamos a revisar algunas técnicas.\n",
    "\n",
    "Vamos a ir desarrollandolas paso a paso y vamos a tratar de dilucidar entre todos cual es la mejor opción para cada variables.\n",
    "\n",
    "Recuerda, es importante saber para que lo quieres, y que técnica vas a utilizar, pero hay algunas cosas que son muy importantes para casi todo algoritmo que optimiza parámetros de un modelo basado en datos:\n",
    "\n",
    "1. Los datos que tienen media 0 facilitan mucho el actuar de los algoritmos de optimización.\n",
    "\n",
    "2. Tener variables que sean *adimensionales* y todas en los mismos rangos facilita el aprendizaje o el uso de métodos estadísticos (sobre todo la parte numérica pero no solamente).\n",
    "\n",
    "3. El escalamiento facilita mucho el preprocesamiento.\n",
    "\n",
    "4. La transformación de datos en muchas ocasiones **es parte del modelo**. El escalamiento o la transformación de variables numéricas se realiza desde la limpieza de datos, pero son al fin de cuenta modelos donde se guardan parámetros. Es una de las principales fuentes de error de muchos desarrollos tecnológicos (mas de los que podría uno creer).\n",
    "\n",
    "5. Es importante asegurarse que no hay valores faltantes, o que los algoritmos que utilicemos sean capaces de lidiar con valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgMNGD9L4w5N"
   },
   "outputs": [],
   "source": [
    "# Vamos a quedarnos con las variables numéricas solamente\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "df = df_raw.select_dtypes(include=numerics).copy()\n",
    "\n",
    "#cambia los valores perdidos por 0 en todas las variables\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkHiLxWJwyF6"
   },
   "source": [
    "### Escalamiento\n",
    "\n",
    "El escalamiento de puede hacer de tres maneras básicas:\n",
    "\n",
    "1. *MinMax* \n",
    "2. *MaxAbs*\n",
    "3. *Z-score* o *estandarización*\n",
    "\n",
    "\n",
    "**Ejercicio: ¿Cuales variables serían las mejor adaptadas para utilizar un escalamiento simple? Revisa que pasa con diferentes variables. Escribe aquí cuales variables parecen beneficiarse de un escalamiento de este tipo y cuales no, agrega aqui abajo tus comentarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sc_HbnhMwxN6"
   },
   "outputs": [],
   "source": [
    "scalers = [\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    preprocessing.MaxAbsScaler(),\n",
    "    preprocessing.StandardScaler()\n",
    "]\n",
    "nombres = ['Min-Max', 'Max-Abs', 'Z-score']\n",
    "\n",
    "variable = 'median_house_value'\n",
    "df_scalers = df.loc[:, [variable]]\n",
    "\n",
    "for (nombre, scaler) in zip(nombres, scalers):\n",
    "    df_scalers[nombre] = scaler.fit_transform(\n",
    "        df_scalers[variable].values.reshape(-1,1)\n",
    "    )\n",
    "\n",
    "df_scalers.hist(bins=100, grid=False, figsize=(15, 10))\n",
    "print(\"Histogramas de acuerdo a 3 tipos de escalamiento\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDmbd_1g96hc"
   },
   "source": [
    "### Transformaciones logarítmicas\n",
    "\n",
    "En muchas ocasiones hay variables con comportamientos parecidos a la exponencial, por lo que un pretratamiento con el logaritmo suele mejorar mucho la distribucion de los datos.\n",
    "\n",
    "**Ejercicio: ¿En este caso en que variables podría aplicar una transformación logarítmica? ¿Se podría una transformación diferente (i.e. raiz cuadrada o cuadrado)? ¿En que casos? Agrumenta tu respuesta aqui mismo**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtWDcbhl-cG8"
   },
   "outputs": [],
   "source": [
    "variable = 'population'\n",
    "nueva = variable + \"-log\"\n",
    "\n",
    "df[nueva] = np.log(df[variable].values)/np.log(2)\n",
    "\n",
    "df[[variable, nueva]]\\\n",
    "    .hist(bins=100, grid=False, figsize=(15, 5))\n",
    "\n",
    "print(\"Logarítmica\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdTdDE13--J4"
   },
   "source": [
    "### Transformación por mapeo a una distribución normal\n",
    "\n",
    "Los métodos principales para una transformación no lineal, con el fin que los datos se asemejen a una distribución normal son los sguientes:\n",
    "\n",
    "- Método de [Box-Cox](http://www.econ.illinois.edu/~econ508/Papers/boxcox64.pdf). El tradicional, pero que sólo se usa en variables con valores positivos.\n",
    "- Método de [Yeo-Johnson](https://link.springer.com/article/10.1007/s10994-021-05960-5). En el enlace hablan del método como el método básico y proponen otro método, se ve interesante pero solo le di una lectura muy superficial.\n",
    "- Método de [Transformación por función de quantil](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2921808/). Algo complicado y no se que tan buenos resultados tenga para preprocesar datos para entrenamiento de modelos.\n",
    "\n",
    "¿Que tan bien funcionan los diferentes métodos para modificar variables antes de un proceso de aprendizaje? En [este artículo](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwipwtHCuPL5AhVgC0QIHRLMAEcQFnoECAYQAQ&url=https%3A%2F%2Fwww.mdpi.com%2F2227-7080%2F9%2F3%2F52%2Fpdf%3Fversion%3D1627349465&usg=AOvVaw2j6S3Ho-qTSauAFL6Euldu) hicieron algunos experimentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4H3k-wWqtaH"
   },
   "outputs": [],
   "source": [
    "normal_dist = [\n",
    "    preprocessing.PowerTransformer(method='box-cox'),\n",
    "    preprocessing.PowerTransformer(),\n",
    "    preprocessing.QuantileTransformer(output_distribution='normal')\n",
    "]\n",
    "nombres = ['Box-Cox', 'Yeo-Johnson', 'Quantile Normal']\n",
    "\n",
    "variable = 'median_income'\n",
    "df_transformer = df.loc[:, [variable]]\n",
    "\n",
    "for (nombre, transformer) in zip(nombres, normal_dist):\n",
    "    df_transformer[nombre] = transformer.fit_transform(\n",
    "        df_transformer[variable].values.reshape(-1,1)\n",
    "    )\n",
    "\n",
    "df_transformer.hist(bins=100, grid=False, figsize=(15, 10))\n",
    "print(\"Histogramas de acuerdo a 3 tipos de transformación a distribución normal\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehc9DGpVB6uJ"
   },
   "source": [
    "**Ejercicio: Revisa con las diferentes variables cuales se podrían ver beneficiadas de una de estas transformaciones y cuales no. Escribe tu respuesta y argumenta tus resultados aquí mismo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFehQP3p2luL"
   },
   "source": [
    "## Discretización\n",
    "\n",
    "Tambien conocida como *bucketting* o *binning* es un método para convertir una variable numérica en discreta por regiones. \n",
    "\n",
    "¿Y para qué quisiersamos discretizar una variables? Porque hay casos donde las variables en forma numérica no nos permiten extraer fácilmente características importantes. Esto es particularmente cierto en variables con una distribución multimodal.\n",
    "\n",
    "Por ejemplo, veamos la variable *latitude*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Hl4rjUZ4cLe"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_raw.latitude.hist(bins=100, grid=False, figsize=(15, 5))\n",
    "print(\"Histograma\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RPXJ2or5Xzu"
   },
   "source": [
    "Podemos ver como hay varios picos. Si revisamos un poco, vemos que los picos representan las latitudes de diferentes ciudades grandes de California. Esto implica que nuestro dato numñerico, en realidad está representando una situación que es de orden cualitativo.\n",
    "\n",
    "Hacer *binning* implica prácticamente separar en intervalos los datos y asignarles un valor cualitativo a cada uno. Esta separación de puede hacer de 3 tipos:\n",
    "\n",
    "1. Por intervalos uniformes\n",
    "2. Por cuantiles\n",
    "3. Por algun método de aprendizaje no supervisado (K-medias es el más sencillo de implementar).\n",
    "\n",
    "Vamos a ver las diferencias entre los tres métodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvAEF66E6NIy"
   },
   "outputs": [],
   "source": [
    "\n",
    "nbins = 7\n",
    "estrategias = ['uniform', 'quantile', 'kmeans']\n",
    "binarizers = [\n",
    "    preprocessing.KBinsDiscretizer(\n",
    "        n_bins=nbins, encode='ordinal', strategy=estrategia\n",
    "    )\n",
    "    for estrategia in estrategias]\n",
    "\n",
    "variable = 'latitude'\n",
    "df_bin = df[[variable]].copy()\n",
    "\n",
    "for (estrategia, binarizer) in zip(estrategias, binarizers):\n",
    "  df_bin[estrategia] = binarizer.fit_transform(\n",
    "      df_bin[variable].values.reshape(-1,1)\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBFmoaVA7u3T"
   },
   "source": [
    "Si no le dices que la codificación es `ordinal`, entonces te lo genera como codificación `one-hot` (que vamos a ver en el manejo de datos cualitativos).\n",
    "\n",
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYkIHGPt8KiK"
   },
   "outputs": [],
   "source": [
    "a = preprocessing.KBinsDiscretizer(n_bins=3, strategy='kmeans')\\\n",
    "    .fit_transform(df_bin.latitude.values.reshape(-1,1))\n",
    "\n",
    "print(\"Un pedazo del resultado (que se encuentra como matriz dispersa)\")\n",
    "print(a[11700:11710,:].toarray())\n",
    "\n",
    "print(\"\\nNúmero de casos por bin (o bucket)\")\n",
    "a.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApkFVkMb_E-_"
   },
   "source": [
    "Aqui podemos ver como nos quedo la binarizacion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TD-o9SWCb_xm"
   },
   "outputs": [],
   "source": [
    "# Esto es pura graficación\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "fig, axs = plt.subplots(2, 2, tight_layout=True, figsize= (15, 8)) \n",
    "\n",
    "for i in range(4):\n",
    "  ax = plt.subplot(2, 2, i+1)\n",
    "  N, bins, patches = ax.hist(df_bin[variable], bins=100)\n",
    "  if i > 0:\n",
    "    ax.vlines(binarizers[i-1].bin_edges_[0], 0, N.max(), lw=1)\n",
    "  #ax.grid(0) \n",
    "  plt.title(df_bin.columns[i])\n",
    "  #plt.ylabel('Counts')\n",
    "  #plt.xlabel(variable)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L133hCxm7MKX"
   },
   "source": [
    "**Ejercicio: ¿Qué pasa si aumentan los *buckets* a 50 por ejemplo? ¿Porqué crees que es mejor mantener un número pequeño de *buquets*? ¿En que variables crees que podría haber una ventaja si se binariza? ¿Porqué? Escribe aqui mismo tus respuestas.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "numericos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "266c02d0b88fb79ac68216b08bc6bf334e56f5daeb776843302a4ad1205260c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
